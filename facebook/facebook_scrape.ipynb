{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c15fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3064bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "fb_handles = df[df['platform'] == 'Facebook']['handle'].to_list()\n",
    "len(fb_handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85012fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# from playwright.async_api import async_playwright\n",
    "\n",
    "# async def scrape_facebook_like_number(username):\n",
    "#     url = f\"https://www.facebook.com/{username}/\"\n",
    "\n",
    "#     async with async_playwright() as p:\n",
    "#         browser = await p.chromium.launch(headless=True)\n",
    "#         page = await browser.new_page(user_agent=\n",
    "#             \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "#             \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"\n",
    "#         )\n",
    "\n",
    "#         await page.goto(url, wait_until=\"networkidle\")\n",
    "\n",
    "#         try:\n",
    "#             await page.locator('div[aria-label=\"Close\"]').first.click()\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         strongs = page.locator(\"strong.html-strong\")\n",
    "#         strong_texts = await strongs.all_text_contents()\n",
    "\n",
    "#         await browser.close()\n",
    "\n",
    "#         for s in strong_texts:\n",
    "#             s_clean = s.strip()\n",
    "#             if s_clean.replace(\"K\",\"\").replace(\"M\",\"\").replace(\".\", \"\").isdigit():\n",
    "#                 return s_clean\n",
    "\n",
    "#         return None\n",
    "\n",
    "\n",
    "# result = await scrape_facebook_like_number(\"DunleavyGovernor\")\n",
    "# result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "\n",
    "def load_facebook_cache(cache_file='facebook_cache.csv'):\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            cache_df = pd.read_csv(cache_file)\n",
    "            print(f\"‚úì Loaded {len(cache_df)} cached profiles from {cache_file}\")\n",
    "            return cache_df\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load cache file: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(f\"No cache file found. Will create {cache_file}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_facebook_cache(data, cache_file='facebook_cache.csv'):\n",
    "    try:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(cache_file, index=False)\n",
    "        print(f\"‚úì Saved {len(df)} profiles to {cache_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save cache: {e}\")\n",
    "\n",
    "async def scrape_facebook_like_number(username):\n",
    "    url = f\"https://www.facebook.com/{username}/\"\n",
    "\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page(user_agent=\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "\n",
    "            await page.goto(url, wait_until=\"networkidle\", timeout=30000)\n",
    "\n",
    "            try:\n",
    "                await page.locator('div[aria-label=\"Close\"]').first.click(timeout=2000)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            strongs = page.locator(\"strong.html-strong\")\n",
    "            strong_texts = await strongs.all_text_contents()\n",
    "\n",
    "            await browser.close()\n",
    "\n",
    "            for s in strong_texts:\n",
    "                s_clean = s.strip()\n",
    "                if s_clean.replace(\"K\",\"\").replace(\"M\",\"\").replace(\".\",\"\").replace(\",\",\"\").isdigit():\n",
    "                    return s_clean\n",
    "\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {username}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def scrape_all_facebook_pages(handles, cache_file='facebook_cache.csv', use_cache=True, delay_min=2, delay_max=10):\n",
    "    # Load cache\n",
    "    cache_df = load_facebook_cache(cache_file) if use_cache else pd.DataFrame()\n",
    "    \n",
    "    results = []\n",
    "    cached_handles = set(cache_df['username'].tolist()) if not cache_df.empty and 'username' in cache_df.columns else set()\n",
    "    \n",
    "    # Add cached data to results\n",
    "    if use_cache and not cache_df.empty:\n",
    "        for handle in handles:\n",
    "            if handle in cached_handles:\n",
    "                cached_row = cache_df[cache_df['username'] == handle].iloc[0].to_dict()\n",
    "                results.append(cached_row)\n",
    "    \n",
    "    # Get handles that need to be fetched\n",
    "    handles_to_fetch = [h for h in handles if h not in cached_handles]\n",
    "    \n",
    "    if handles_to_fetch:\n",
    "        print(f\"\\nüìä Need to fetch {len(handles_to_fetch)} new profiles\")\n",
    "        print(f\"‚úÖ Using {len(cached_handles)} cached profiles\\n\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All {len(handles)} profiles loaded from cache!\")\n",
    "        return results\n",
    "    \n",
    "    # Fetch new data with progress bar\n",
    "    for handle in tqdm(handles_to_fetch, desc=\"Scraping Facebook pages\"):\n",
    "        like_count = await scrape_facebook_like_number(handle)\n",
    "        \n",
    "        data = {\n",
    "            'username': handle,\n",
    "            'like_count': like_count,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        results.append(data)\n",
    "        \n",
    "        if like_count:\n",
    "            tqdm.write(f\"‚úì {handle}: {like_count} likes\")\n",
    "        else:\n",
    "            tqdm.write(f\"‚úó {handle}: Failed to retrieve data\")\n",
    "        \n",
    "        # Save cache after each successful fetch\n",
    "        save_facebook_cache(results, cache_file)\n",
    "        \n",
    "        # Add random delay between requests\n",
    "        if handle != handles_to_fetch[-1]:\n",
    "            delay = random.uniform(delay_min, delay_max)\n",
    "            tqdm.write(f\"‚è±Ô∏è  Waiting {delay:.1f} seconds...\")\n",
    "            await asyncio.sleep(delay)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26cd2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 182 cached profiles from facebook_cache.csv\n",
      "\n",
      "üìä Need to fetch 369 new profiles\n",
      "‚úÖ Using 182 cached profiles\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971c032535fc4528849281a926a131c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Facebook pages:   0%|          | 0/369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úó RepSchneider: Failed to retrieve data\n",
      "‚úì Saved 183 profiles to facebook_cache.csv\n",
      "‚è±Ô∏è  Waiting 2.3 seconds...\n",
      "‚úó OfficialRepDannyDavis: Failed to retrieve data\n",
      "‚úì Saved 184 profiles to facebook_cache.csv\n",
      "‚è±Ô∏è  Waiting 8.3 seconds...\n",
      "‚úó OfficialRepDannyDavis: Failed to retrieve data\n",
      "‚úì Saved 184 profiles to facebook_cache.csv\n",
      "‚è±Ô∏è  Waiting 8.3 seconds...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fb_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scrape_all_facebook_pages(\n\u001b[1;32m      2\u001b[0m     fb_handles, \n\u001b[1;32m      3\u001b[0m     cache_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook_cache.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     delay_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# Minimum delay in seconds\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     delay_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Maximum delay in seconds (random delay between 2-10 seconds)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "Cell \u001b[0;32mIn[12], line 118\u001b[0m, in \u001b[0;36mscrape_all_facebook_pages\u001b[0;34m(handles, cache_file, use_cache, delay_min, delay_max)\u001b[0m\n\u001b[1;32m    116\u001b[0m         delay \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(delay_min, delay_max)\n\u001b[1;32m    117\u001b[0m         tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚è±Ô∏è  Waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelay\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(delay)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/asyncio/tasks.py:605\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    601\u001b[0m h \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mcall_later(delay,\n\u001b[1;32m    602\u001b[0m                     futures\u001b[38;5;241m.\u001b[39m_set_result_unless_cancelled,\n\u001b[1;32m    603\u001b[0m                     future, result)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m     h\u001b[38;5;241m.\u001b[39mcancel()\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fb_data = await scrape_all_facebook_pages(\n",
    "    fb_handles, \n",
    "    cache_file='facebook_cache.csv', \n",
    "    use_cache=True,\n",
    "    delay_min=2,  \n",
    "    delay_max=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079353f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from results\n",
    "df_facebook = pd.DataFrame(fb_data)\n",
    "df_facebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
